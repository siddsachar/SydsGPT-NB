{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364e9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec6ca1",
   "metadata": {},
   "source": [
    "# SYD'S GPT-345M Model Configuration\n",
    "\n",
    "This notebook cell defines the configuration dictionary for a GPT-style language model with approximately 345 million parameters. The configuration parameters are as follows:\n",
    "\n",
    "- **vocab_size**: The number of unique tokens (words, subwords, or characters) that the model can understand. For GPT-345M, this is set to 50,257, matching the standard GPT-2 vocabulary size.\n",
    "- **context_length**: The maximum number of tokens the model can consider in a single input sequence. Here, it is set to 1,024 tokens, which determines the model's memory window for generating or analyzing text.\n",
    "- **embedding_dim**: The size of the vector used to represent each token. A higher embedding dimension allows the model to capture more nuanced relationships between tokens. This configuration uses 1,024 dimensions.\n",
    "- **num_heads**: The number of attention heads in the multi-head self-attention mechanism. More heads allow the model to focus on different parts of the input simultaneously. This model uses 16 heads.\n",
    "- **num_layers**: The number of transformer blocks (layers) stacked in the model. More layers generally increase the model's capacity to learn complex patterns. Here, 24 layers are used.\n",
    "- **dropout**: The dropout rate applied during training to prevent overfitting. A value of 0.1 means 10% of the connections are randomly dropped during each training step.\n",
    "- **qkv_bias**: A boolean indicating whether to include a bias term in the query, key, and value projections of the attention mechanism. Setting this to `False` means no bias is added.\n",
    "\n",
    "This configuration is typical for a medium-sized GPT-2 model and can be used as a starting point for training or fine-tuning a transformer-based language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddaabe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYDSGPT_CONFIG_345M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"embedding_dim\" : 1024,\n",
    "    \"num_heads\" : 16,\n",
    "    \"num_layers\" : 24,\n",
    "    \"dropout\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed747498",
   "metadata": {},
   "source": [
    "# PlaceholderGPT Model Class Overview\n",
    "\n",
    "The following code cell defines a simplified, illustrative version of a GPT-style transformer model using PyTorch. This implementation is intended for educational purposes and demonstrates the core architectural components of a transformer-based language model:\n",
    "\n",
    "## Main Components\n",
    "\n",
    "- **PlaceholderGPT (nn.Module)**: The main model class. It initializes the following layers:\n",
    "  - **Token Embedding**: Maps each token in the input sequence to a high-dimensional vector.\n",
    "  - **Position Embedding**: Adds positional information to each token, allowing the model to distinguish between tokens at different positions.\n",
    "  - **Dropout Layer**: Regularizes the model by randomly dropping units during training to prevent overfitting.\n",
    "  - **Transformer Blocks**: A stack of placeholder transformer blocks (not fully implemented here) that would normally perform self-attention and feed-forward operations.\n",
    "  - **Final Layer Normalization**: Normalizes the output of the transformer blocks.\n",
    "  - **Output Projection**: Projects the final hidden states to the vocabulary size, producing logits for each token.\n",
    "\n",
    "- **forward(input)**: The forward pass of the model. It processes the input sequence through embeddings, dropout, transformer blocks, layer normalization, and output projection to produce logits for each token position.\n",
    "\n",
    "## Supporting Classes\n",
    "\n",
    "- **PlaceholderTransformerBlock (nn.Module)**: Represents a single transformer block. In a full implementation, this would include multi-head self-attention and feed-forward layers. Here, it simply returns the input unchanged.\n",
    "\n",
    "- **PlaceholderLayerNorm (nn.Module)**: A placeholder for layer normalization. In a real model, this would normalize the input tensor across the embedding dimension.\n",
    "\n",
    "### What does `eps = 1e-5` mean?\n",
    "\n",
    "In the context of neural networks and layer normalization, `eps` (epsilon) is a small constant added to the denominator when normalizing activations. This prevents division by zero and improves numerical stability.\n",
    "\n",
    "- **Value:** `1e-5` means epsilon is set to $0.00001$.\n",
    "- **Purpose:** When calculating the normalized output, the formula typically looks like:\n",
    "  $$\n",
    "  \\text{normalized} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "  $$\n",
    "  where $\\mu$ is the mean and $\\sigma^2$ is the variance of the input $x$.\n",
    "- **Why needed:** If the variance is very close to zero, adding epsilon ensures the denominator is never zero, avoiding undefined results and improving stability during training.\n",
    "\n",
    "This is a standard practice in deep learning layers such as batch normalization and layer normalization.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This code is a skeleton and does not implement the full transformer logic (e.g., attention mechanisms, feed-forward networks, or actual layer normalization).\n",
    "- The model uses configuration parameters such as `vocab_size`, `embedding_dim`, `context_length`, `num_layers`, and `dropout` from the configuration dictionary defined earlier in the notebook.\n",
    "- The purpose of this cell is to illustrate the structure and flow of a transformer-based language model in PyTorch, serving as a starting point for further development or experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d152a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceholderGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"embedding_dim\"])\n",
    "        self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"embedding_dim\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        self.transformer_blocks = nn.Sequential(*[PlaceholderTransformerBlock(config) for _ in range(config[\"num_layers\"])])\n",
    "        self.final_layer_norm = PlaceholderLayerNorm(config[\"embedding_dim\"])\n",
    "        self.output_projection = nn.Linear(config[\"embedding_dim\"], config[\"vocab_size\"], bias = False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_length = input.shape\n",
    "        token_embeddings = self.token_embedding(input)\n",
    "        position_embeddings = self.position_embedding(torch.arange(seq_length, device = input.device))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        return logits\n",
    "    \n",
    "class PlaceholderTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class PlaceholderLayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b9463",
   "metadata": {},
   "source": [
    "# Example: Tokenization and Model Forward Pass\n",
    "\n",
    "This code cell demonstrates how to tokenize input text, prepare a batch for the model, and run a forward pass using the `PlaceholderGPT` model defined earlier in the notebook.\n",
    "\n",
    "## Steps Explained\n",
    "\n",
    "1. **Tokenization**\n",
    "   - Uses the `tiktoken` library to obtain the GPT-2 tokenizer.\n",
    "   - Two example sentences (`ex1` and `ex2`) are encoded into lists of token IDs.\n",
    "\n",
    "2. **Batch Preparation**\n",
    "   - The encoded token lists are converted to PyTorch tensors and added to a batch list.\n",
    "   - The batch is stacked into a single tensor with shape `(batch_size, sequence_length)`.\n",
    "   - The batch tensor is printed to show its contents.\n",
    "\n",
    "3. **Model Initialization and Forward Pass**\n",
    "   - Sets a manual random seed for reproducibility.\n",
    "   - Instantiates the `PlaceholderGPT` model with the configuration dictionary.\n",
    "   - Runs the batch through the model to obtain logits (unnormalized predictions for each token position).\n",
    "\n",
    "4. **Output**\n",
    "   - Prints the shape of the logits tensor, which should be `(batch_size, sequence_length, vocab_size)`.\n",
    "   - Prints the actual logits tensor values.\n",
    "\n",
    "## Notes\n",
    "- This example uses a placeholder model, so the logits are not meaningful predictions but demonstrate the expected output structure.\n",
    "- The code illustrates the typical workflow for preparing text data and running it through a transformer-based language model in PyTorch.\n",
    "- The use of `torch.manual_seed` ensures that results are reproducible for debugging and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b808240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   703,   389,   345],\n",
      "        [ 2061,   389,   345,  1804]])\n",
      "Logits Shape: torch.Size([2, 4, 50257])\n",
      "Logits: \n",
      " tensor([[[ 0.2638, -0.1249, -1.9838,  ..., -0.0289,  0.6842, -0.1247],\n",
      "         [-0.0896,  1.0701,  0.0236,  ...,  0.4069, -0.5886, -0.4666],\n",
      "         [ 0.6501,  0.0914, -1.0634,  ...,  1.0771,  0.1660, -0.0224],\n",
      "         [ 0.5803,  0.3458, -0.4553,  ...,  0.7233, -1.3835, -0.2096]],\n",
      "\n",
      "        [[ 1.6676,  0.5273, -0.3480,  ..., -0.0478, -0.2177,  0.0361],\n",
      "         [ 0.6377, -0.6156, -1.3517,  ...,  1.0306, -1.0850, -1.3458],\n",
      "         [ 0.1607,  0.2084,  0.6130,  ...,  0.6961, -0.9398,  0.6000],\n",
      "         [-0.1086, -0.2255, -0.4622,  ...,  0.8606,  0.1524, -1.0448]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Logits Shape: torch.Size([2, 4, 50257])\n",
      "Logits: \n",
      " tensor([[[ 0.2638, -0.1249, -1.9838,  ..., -0.0289,  0.6842, -0.1247],\n",
      "         [-0.0896,  1.0701,  0.0236,  ...,  0.4069, -0.5886, -0.4666],\n",
      "         [ 0.6501,  0.0914, -1.0634,  ...,  1.0771,  0.1660, -0.0224],\n",
      "         [ 0.5803,  0.3458, -0.4553,  ...,  0.7233, -1.3835, -0.2096]],\n",
      "\n",
      "        [[ 1.6676,  0.5273, -0.3480,  ..., -0.0478, -0.2177,  0.0361],\n",
      "         [ 0.6377, -0.6156, -1.3517,  ...,  1.0306, -1.0850, -1.3458],\n",
      "         [ 0.1607,  0.2084,  0.6130,  ...,  0.6961, -0.9398,  0.6000],\n",
      "         [-0.1086, -0.2255, -0.4622,  ...,  0.8606,  0.1524, -1.0448]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "ex1 = \"Hello how are you\"\n",
    "ex2 = \"What are you doing\"\n",
    "batch.append(torch.tensor(tokenizer.encode(ex1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(ex2)))\n",
    "batch = torch.stack(batch, dim = 0)\n",
    "print(batch)\n",
    "\n",
    "torch.manual_seed(246)\n",
    "test_model = PlaceholderGPT(SYDSGPT_CONFIG_345M)\n",
    "logits = test_model(batch)\n",
    "print(f\"Logits Shape: {logits.shape}\")\n",
    "print(f\"Logits: \\n {logits}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f087c",
   "metadata": {},
   "source": [
    "# Example: Manual Layer Normalization in PyTorch\n",
    "\n",
    "This code cell demonstrates how to manually normalize the output of a neural network layer in PyTorch, illustrating the concept behind layer normalization.\n",
    "\n",
    "## Steps Explained\n",
    "\n",
    "1. **Batch and Layer Setup**\n",
    "   - Sets a manual random seed for reproducibility.\n",
    "   - Creates a random batch tensor `ex_batch` of shape `(2, 6)`.\n",
    "   - Defines a simple neural network layer (`nn_layer`) consisting of a linear transformation followed by a ReLU activation.\n",
    "   - Passes the batch through the layer to obtain `output`.\n",
    "   - Prints the shape and values of the output tensor.\n",
    "\n",
    "2. **Mean and Variance Calculation**\n",
    "   - Computes the mean and variance of the output tensor along the last dimension (features) for each sample in the batch.\n",
    "   - Prints the mean and variance values.\n",
    "\n",
    "3. **Manual Normalization**\n",
    "   - Normalizes the output tensor by subtracting the mean and dividing by the square root of the variance (without epsilon for simplicity).\n",
    "   - Prints the normalized output tensor.\n",
    "   - Calculates and prints the mean and variance of the normalized output to verify that the mean is close to zero and the variance is close to one for each sample.\n",
    "\n",
    "## Notes\n",
    "- This example shows the core idea behind layer normalization, which is commonly used in deep learning models to stabilize and accelerate training.\n",
    "- In practice, a small epsilon value is added to the denominator to prevent division by zero and improve numerical stability.\n",
    "- The normalization is performed per sample (row) in the batch, across the feature dimension.\n",
    "- This manual approach helps illustrate what built-in PyTorch layers like `nn.LayerNorm` do internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ce5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([2, 8])\n",
      "Output: \n",
      " tensor([[0.0000, 1.4358, 0.8909, 0.0000, 0.3553, 1.4952, 0.0000, 0.0000],\n",
      "        [1.4236, 0.0000, 0.0000, 2.0742, 1.7022, 0.1547, 0.0589, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean: \n",
      " tensor([[0.5221],\n",
      "        [0.6767]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[0.4337],\n",
      "        [0.7986]], grad_fn=<VarBackward0>)\n",
      "Normalized Output: \n",
      " tensor([[-0.7929,  1.3874,  0.5599, -0.7929, -0.2533,  1.4775, -0.7929, -0.7929],\n",
      "        [ 0.8357, -0.7572, -0.7572,  1.5638,  1.1475, -0.5841, -0.6913, -0.7572]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean After Norm: \n",
      " tensor([[0.0000e+00],\n",
      "        [2.2352e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance After Norm: \n",
      " tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(246)\n",
    "ex_batch = torch.randn(2,6)\n",
    "nn_layer = nn.Sequential(nn.Linear(6,8), nn.ReLU())\n",
    "output = nn_layer(ex_batch)\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Output: \\n {output}\")\n",
    "\n",
    "mean = output.mean(dim = -1, keepdim = True)\n",
    "variance = output.var(dim = -1, keepdim = True)\n",
    "print(f\"Mean: \\n {mean}\")\n",
    "print(f\"Variance: \\n {variance}\")\n",
    "\n",
    "normalized_output = (output - mean) / torch.sqrt(variance)\n",
    "mean_after_norm = normalized_output.mean(dim = -1, keepdim = True)\n",
    "variance_after_norm = normalized_output.var(dim = -1, keepdim = True)\n",
    "print(f\"Normalized Output: \\n {normalized_output}\")\n",
    "print(f\"Mean After Norm: \\n {mean_after_norm}\")\n",
    "print(f\"Variance After Norm: \\n {variance_after_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbd1e0",
   "metadata": {},
   "source": [
    "# Improved Custom LayerNorm Class in PyTorch\n",
    "\n",
    "This code cell presents an improved custom implementation of the Layer Normalization operation, a key technique for stabilizing and accelerating training in deep learning models, especially transformers.\n",
    "\n",
    "## How This Implementation Works\n",
    "\n",
    "- **Class Definition:**\n",
    "  - `LayerNorm` inherits from `nn.Module` for seamless integration with PyTorch models.\n",
    "  - The constructor accepts `embedding_dim` (the size of the last dimension to normalize) and `eps` (a small constant for numerical stability).\n",
    "\n",
    "- **Parameters:**\n",
    "  - `scale`: A learnable scaling parameter (initialized to ones) that allows the model to adjust the normalized output's scale.\n",
    "  - `shift`: A learnable shifting parameter (initialized to zeros) that allows the model to adjust the normalized output's mean.\n",
    "\n",
    "- **Forward Pass:**\n",
    "  - Calculates the mean and variance of the input tensor `x` along the last dimension for each sample.\n",
    "  - Normalizes the input: $\\text{normalized}_x = \\frac{x - \\text{mean}}{\\sqrt{\\text{variance} + \\epsilon}}$\n",
    "  - Applies the learnable scale (`scale`) and shift (`shift`) to the normalized tensor.\n",
    "  - Returns the final output.\n",
    "\n",
    "## Key Points\n",
    "- Layer normalization is performed per sample, across the feature dimension, making it suitable for variable-length sequences and transformer models.\n",
    "- The use of `eps` ensures numerical stability by preventing division by zero.\n",
    "- Learnable parameters (`scale` and `shift`) allow the model to recover the original distribution if needed.\n",
    "- This implementation closely mirrors PyTorch's built-in `nn.LayerNorm`, but is written from scratch for educational clarity and flexibility.\n",
    "\n",
    "## Usage\n",
    "This custom `LayerNorm` class can be used in place of PyTorch's built-in layer normalization in neural network modules, especially when you want to understand, customize, or experiment with normalization behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b6d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(embedding_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embedding_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        variance = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        normalized_x = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        return self.scale * normalized_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65028f",
   "metadata": {},
   "source": [
    "# Using the Custom LayerNorm Class\n",
    "\n",
    "This code cell demonstrates how to use the custom `LayerNorm` class defined above to normalize a batch of data in PyTorch.\n",
    "\n",
    "## Steps Explained\n",
    "\n",
    "1. **LayerNorm Instantiation**\n",
    "   - Creates an instance of the custom `LayerNorm` class with `embedding_dim = 6`, matching the feature dimension of the input batch (`ex_batch`).\n",
    "\n",
    "2. **Applying Layer Normalization**\n",
    "   - Passes the batch tensor `ex_batch` through the `LayerNorm` instance to obtain `normalized_output`.\n",
    "   - Prints the normalized output tensor.\n",
    "\n",
    "3. **Verifying Normalization**\n",
    "   - Calculates and prints the mean and variance of the normalized output along the last dimension for each sample in the batch.\n",
    "   - The mean should be close to zero and the variance close to one, confirming that the normalization is working as intended (modulo learnable parameters, which are initialized to scale=1 and shift=0).\n",
    "\n",
    "## Notes\n",
    "- This example shows how to use a custom normalization layer in practice, similar to how you would use PyTorch's built-in `nn.LayerNorm`.\n",
    "- Layer normalization is applied independently to each sample (row) in the batch, across the feature dimension.\n",
    "- The learnable parameters (`scale` and `shift`) allow the model to adapt the normalized output during training.\n",
    "- This approach is essential in transformer models and other deep learning architectures to ensure stable and efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32040b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Norm Output: \n",
      " tensor([[-1.3450,  1.1168, -0.1748, -0.5987, -0.5124,  1.5140],\n",
      "        [-1.2331,  0.8340,  0.8506,  1.1080, -0.2246, -1.3349]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Mean After Layer Norm: \n",
      " tensor([[-3.9736e-08],\n",
      "        [-1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance After Layer Norm: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = LayerNorm(embedding_dim = 6)\n",
    "normalized_output = layer_norm(ex_batch)\n",
    "print(f\"Layer Norm Output: \\n {normalized_output}\")\n",
    "mean_after_norm = normalized_output.mean(dim = -1, keepdim = True)\n",
    "variance_after_norm = normalized_output.var(dim = -1, keepdim = True, unbiased = False)\n",
    "print(f\"Mean After Layer Norm: \\n {mean_after_norm}\")\n",
    "print(f\"Variance After Layer Norm: \\n {variance_after_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5accf74d",
   "metadata": {},
   "source": [
    "# Custom GELU Activation Function in PyTorch\n",
    " \n",
    "This markdown explains the implementation and purpose of the custom `GELU` (Gaussian Error Linear Unit) activation function defined in the following code cell.\n",
    "\n",
    "## What is GELU?\n",
    "- **GELU** is an activation function commonly used in transformer-based models, such as BERT and GPT.\n",
    "- It is defined as:\n",
    "  $$\n",
    "  \\text{GELU}(x) = 0.5x \\left[1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right)\\right]\n",
    "  $$\n",
    "- GELU provides a smooth, non-linear transformation, allowing the network to learn complex patterns more effectively than traditional activations like ReLU.\n",
    "\n",
    "## Code Explanation\n",
    "- **Class Definition:**\n",
    "  - `GELU` inherits from `nn.Module`, making it compatible with PyTorch models and layers.\n",
    "- **Constructor (`__init__`):**\n",
    "  - Calls the superclass constructor. No additional parameters are needed for GELU.\n",
    "- **Forward Method:**\n",
    "  - Implements the GELU formula using PyTorch operations.\n",
    "  - The expression uses `torch.tanh` and `torch.pow` to compute the non-linear transformation.\n",
    "  - The constant $0.044715$ is an approximation used in the original paper for computational efficiency.\n",
    "\n",
    "## Why Use GELU?\n",
    "- GELU is preferred in modern transformer architectures because:\n",
    "  - It provides smoother gradients than ReLU, improving optimization.\n",
    "  - It enables better performance in deep networks, especially for NLP tasks.\n",
    "  - It is the default activation in models like BERT and GPT-2/3.\n",
    "\n",
    "## Usage Example\n",
    "To use this custom GELU activation in a PyTorch model:\n",
    "```python\n",
    "gelu = GELU()\n",
    "output = gelu(input_tensor)\n",
    "```\n",
    "This can be used as a drop-in replacement for `nn.GELU` or `F.gelu` in custom model architectures.\n",
    "\n",
    "## References\n",
    "- [Original GELU Paper](https://arxiv.org/abs/1606.08415)\n",
    "- [PyTorch Documentation: GELU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "438b110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x *(1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1d2ea",
   "metadata": {},
   "source": [
    "# FeedForward Network in Transformer Models\n",
    " \n",
    "This markdown explains the implementation and role of the `FeedForward` class defined in the following code cell, which is a core component of transformer-based architectures like GPT and BERT.\n",
    "\n",
    "## What is a FeedForward Network?\n",
    "- In transformer models, each transformer block contains a position-wise feedforward network (FFN) that processes each token's embedding independently after the self-attention mechanism.\n",
    "- The FFN typically consists of two linear (fully connected) layers with a non-linear activation function in between.\n",
    "\n",
    "## Code Explanation\n",
    "- **Class Definition:**\n",
    "  - `FeedForward` inherits from `nn.Module`, making it compatible with PyTorch models.\n",
    "  - The constructor takes a `config` dictionary containing model hyperparameters.\n",
    "\n",
    "- **Layers:**\n",
    "  - The FFN is implemented as a `nn.Sequential` container with three layers:\n",
    "    1. `nn.Linear(config[\"embedding_dim\"], 4 * config[\"embedding_dim\"])`: Expands the embedding dimension by a factor of 4 (a common practice in transformer models).\n",
    "    2. `GELU()`: Applies the custom Gaussian Error Linear Unit activation function for non-linearity.\n",
    "    3. `nn.Linear(4 * config[\"embedding_dim\"], config[\"embedding_dim\"])`: Projects the expanded dimension back to the original embedding size.\n",
    "\n",
    "- **Forward Method:**\n",
    "  - The `forward` method passes the input tensor `x` through the sequential layers, returning the transformed output.\n",
    "\n",
    "## Why Use This Structure?\n",
    "- The two-layer FFN allows the model to learn complex transformations for each token independently, increasing the model's capacity and expressiveness.\n",
    "- The use of the GELU activation provides smooth, non-linear behavior, which is beneficial for deep learning optimization.\n",
    "- Expanding and then reducing the embedding dimension enables the network to capture richer representations before projecting back to the original size.\n",
    "\n",
    "## Usage in Transformers\n",
    "- In a transformer block, the FFN is applied after the self-attention mechanism and before the residual connection and layer normalization.\n",
    "- This structure is standard in models like GPT-2, GPT-3, and BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce5939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config[\"embedding_dim\"], 4 * config[\"embedding_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77657b0f",
   "metadata": {},
   "source": [
    "# Example: Using the FeedForward Network\n",
    " \n",
    "This markdown explains the demonstration code for instantiating and running the `FeedForward` network defined earlier in the notebook.\n",
    "\n",
    "## What Does This Code Do?\n",
    "- **Instantiates** the `FeedForward` class using the model configuration dictionary (`SYDSGPT_CONFIG_345M`).\n",
    "- **Creates** an example input tensor of shape `(2, 6, embedding_dim)`, representing a batch of 2 sequences, each with 6 tokens, and each token represented by a vector of size `embedding_dim`.\n",
    "- **Runs** the input through the feedforward network to obtain the output tensor.\n",
    "- **Prints** the shape and values of the output tensor for inspection.\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "1. **FeedForward Instantiation**\n",
    "   - `feed_forward = FeedForward(SYDSGPT_CONFIG_345M)`\n",
    "   - Creates a feedforward network with the same embedding dimension as the model configuration.\n",
    "\n",
    "2. **Example Input Creation**\n",
    "   - `example_input = torch.randn(2, 6, SYDSGPT_CONFIG_345M[\"embedding_dim\"])`\n",
    "   - Generates a random tensor to simulate a batch of token embeddings.\n",
    "   - Shape: `(batch_size, sequence_length, embedding_dim)`.\n",
    "\n",
    "3. **Forward Pass**\n",
    "   - `output = feed_forward(example_input)`\n",
    "   - Passes the input through the feedforward network, applying two linear transformations and a GELU activation in between.\n",
    "\n",
    "4. **Output Inspection**\n",
    "   - Prints the shape and values of the output tensor.\n",
    "   - The output shape matches the input: `(2, 6, embedding_dim)`, since the feedforward network projects back to the original embedding size.\n",
    "\n",
    "## Why Is This Important?\n",
    "- This example shows how to use the feedforward network as part of a transformer block, processing each token's embedding independently.\n",
    "- The feedforward network is a key component in transformer architectures, enabling the model to learn complex, non-linear transformations for each token.\n",
    "- Understanding the input and output shapes is crucial for integrating the feedforward network into larger models.\n",
    "\n",
    "## Usage in Practice\n",
    "- In a full transformer model, this feedforward network would be used after the self-attention mechanism and before layer normalization and residual connections.\n",
    "- The pattern of expanding and reducing the embedding dimension is standard in transformer-based models like GPT and BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc21b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Forward Output Shape: torch.Size([2, 6, 1024])\n",
      "Feed Forward Output: \n",
      " tensor([[[ 0.0212, -0.2190,  0.2352,  ...,  0.1612,  0.3023, -0.1417],\n",
      "         [-0.2370,  0.0163,  0.2842,  ...,  0.1003,  0.0263, -0.0122],\n",
      "         [-0.1790,  0.0294, -0.0350,  ...,  0.0320,  0.2061, -0.1103],\n",
      "         [ 0.1206,  0.0419,  0.0231,  ..., -0.0099, -0.1882, -0.2501],\n",
      "         [ 0.0942, -0.2093,  0.3045,  ..., -0.2473,  0.1754, -0.1967],\n",
      "         [ 0.0837,  0.1889,  0.1583,  ...,  0.2284,  0.2347, -0.1868]],\n",
      "\n",
      "        [[-0.4378, -0.0083,  0.1454,  ...,  0.0780, -0.0978,  0.0250],\n",
      "         [ 0.0409, -0.1573,  0.2250,  ...,  0.2111, -0.2130,  0.1422],\n",
      "         [ 0.3191, -0.2163,  0.0870,  ..., -0.2538, -0.0757, -0.1970],\n",
      "         [ 0.0511,  0.0946,  0.2757,  ...,  0.0645,  0.1366, -0.1490],\n",
      "         [ 0.2247, -0.1286,  0.1931,  ..., -0.1176,  0.4198,  0.1183],\n",
      "         [ 0.3904, -0.3158,  0.2139,  ...,  0.2581,  0.1807, -0.2763]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "feed_forward = FeedForward(SYDSGPT_CONFIG_345M)\n",
    "example_input = torch.randn(2, 6, SYDSGPT_CONFIG_345M[\"embedding_dim\"])\n",
    "output = feed_forward(example_input)\n",
    "print(f\"Feed Forward Output Shape: {output.shape}\")\n",
    "print(f\"Feed Forward Output: \\n {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b58d4c",
   "metadata": {},
   "source": [
    "# Residual Connections and Gradient Flow: Demonstration\n",
    " \n",
    "This markdown explains the code cell that demonstrates the effect of residual (shortcut) connections on gradient flow in deep neural networks, a key concept in modern architectures like transformers and ResNets.\n",
    "\n",
    "## What Does This Code Do?\n",
    "- **Defines** a test neural network class (`ResidualConnectionsTestNN`) with multiple layers, optionally using residual (shortcut) connections.\n",
    "- **Implements** a function (`get_gradients`) to compute and print the mean absolute value of gradients for each layer's weights after a backward pass.\n",
    "- **Compares** two models: one with residual connections and one without, using the same architecture and input.\n",
    "- **Prints** the mean of gradients for each layer in both cases, illustrating the impact of residual connections on gradient propagation.\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "1. **ResidualConnectionsTestNN Class**\n",
    "   - Takes a list of layer dimensions and a boolean `use_shortcuts` to control whether residual connections are used.\n",
    "   - Builds a sequence of layers, each consisting of a linear transformation followed by a GELU activation.\n",
    "   - In the `forward` method, if `use_shortcuts` is `True` and the input and output shapes match, adds the input to the output (residual connection). Otherwise, just uses the output.\n",
    "\n",
    "2. **get_gradients Function**\n",
    "   - Runs a forward pass, computes a simple MSE loss against a target, and performs backpropagation.\n",
    "   - Prints the mean absolute value of the gradients for each weight parameter in the model.\n",
    "\n",
    "3. **Experiment Setup**\n",
    "   - Defines a simple input tensor and a list of layer dimensions for a 6-layer network.\n",
    "   - Instantiates two models with the same random seed: one with residual connections, one without.\n",
    "   - Runs the gradient computation for both models and prints the results.\n",
    "\n",
    "## Why Are Residual Connections Important?\n",
    "- **Gradient Flow:** Residual connections help gradients flow backward through deep networks, reducing the risk of vanishing gradients and enabling the training of much deeper models.\n",
    "- **Stability:** They make optimization easier and more stable, especially in very deep architectures.\n",
    "- **Standard Practice:** Residual connections are a standard component in transformer models (e.g., GPT, BERT) and ResNets.\n",
    "\n",
    "## What Should You Observe?\n",
    "- The mean of gradients in the model **with** residual connections should generally be larger and more evenly distributed across layers, indicating better gradient flow.\n",
    "- In the model **without** residuals, gradients may diminish rapidly in deeper layers, illustrating the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4abb7bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients without Residual Connections:\n",
      "Mean of Gradients for layers.0.0.weight: 2.173086795664858e-05\n",
      "Mean of Gradients for layers.1.0.weight: 1.2031879123242106e-05\n",
      "Mean of Gradients for layers.2.0.weight: 4.762777825817466e-05\n",
      "Mean of Gradients for layers.3.0.weight: 0.0004226445162203163\n",
      "Mean of Gradients for layers.4.0.weight: 0.0009699637885205448\n",
      "Mean of Gradients for layers.5.0.weight: 0.0055029913783073425\n",
      "Gradients with Residual Connections:\n",
      "Mean of Gradients for layers.0.0.weight: 0.0026402678340673447\n",
      "Mean of Gradients for layers.1.0.weight: 0.0009344664285890758\n",
      "Mean of Gradients for layers.2.0.weight: 0.004430014174431562\n",
      "Mean of Gradients for layers.3.0.weight: 0.00808763224631548\n",
      "Mean of Gradients for layers.4.0.weight: 0.004049395211040974\n",
      "Mean of Gradients for layers.5.0.weight: 0.06586597859859467\n"
     ]
    }
   ],
   "source": [
    "class ResidualConnectionsTestNN(nn.Module):\n",
    "    def __init__(self, layer_dims, use_shortcuts):\n",
    "        super().__init__()\n",
    "        self.use_shortcuts = use_shortcuts\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_dims[0], layer_dims[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_dims[1], layer_dims[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_dims[2], layer_dims[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_dims[3], layer_dims[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_dims[4], layer_dims[5]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_dims[5], layer_dims[6]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcuts and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "def get_gradients(model, input):\n",
    "    output = model(input)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss_function = nn.MSELoss()\n",
    "    loss = loss_function(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"Mean of Gradients for {name}: {param.grad.abs().mean().item()}\")\n",
    "\n",
    "\n",
    "\n",
    "layer_dims = [3, 3, 3, 3, 3, 3, 1]\n",
    "input = torch.tensor([[-1., 0., 1.]])\n",
    "\n",
    "torch.manual_seed(246)\n",
    "model_without_residuals = ResidualConnectionsTestNN(layer_dims, use_shortcuts = False)\n",
    "torch.manual_seed(246)\n",
    "model_with_residuals = ResidualConnectionsTestNN(layer_dims, use_shortcuts = True)\n",
    "\n",
    "print(\"Gradients without Residual Connections:\")\n",
    "get_gradients(model_without_residuals, input)\n",
    "\n",
    "print(\"Gradients with Residual Connections:\")\n",
    "get_gradients(model_with_residuals, input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6719aa",
   "metadata": {},
   "source": [
    "# Transformer Block: Architecture and Residual Connections\n",
    " \n",
    "This markdown explains the implementation and design of the `TransformerBlock` class, a core building block in transformer-based models such as GPT and BERT.\n",
    "\n",
    "## Overview\n",
    "- The transformer block combines multi-head self-attention, feedforward networks, layer normalization, dropout, and residual (shortcut) connections.\n",
    "- This structure enables the model to learn complex dependencies and representations efficiently, while maintaining stable training in deep architectures.\n",
    "\n",
    "## Components Explained\n",
    "1. **Multi-Head Attention (`self.attention`)**\n",
    "   - Uses the imported `MultiHeadAttention` module to perform self-attention across the input sequence.\n",
    "   - Allows the model to focus on different parts of the sequence simultaneously, capturing various relationships between tokens.\n",
    "   - Configured with input/output dimensions, dropout, context length, number of heads, and optional bias.\n",
    "\n",
    "2. **Layer Normalization (`self.layer_norm1`, `self.layer_norm2`)**\n",
    "   - Normalizes the input to each sub-layer, stabilizing and accelerating training.\n",
    "   - Applied before both the attention and feedforward sub-layers (pre-norm architecture).\n",
    "\n",
    "3. **FeedForward Network (`self.feed_forward`)**\n",
    "   - Applies a position-wise feedforward network to each token embedding independently.\n",
    "   - Consists of two linear layers with a GELU activation in between (see earlier notebook cells for details).\n",
    "\n",
    "4. **Dropout (`self.dropout`)**\n",
    "   - Randomly zeroes some elements of the input tensor during training, helping prevent overfitting.\n",
    "   - Applied after both the attention and feedforward sub-layers.\n",
    "\n",
    "5. **Residual Connections**\n",
    "   - Adds the input of each sub-layer to its output (\"shortcut\"), enabling better gradient flow and more stable optimization.\n",
    "   - Two residual connections: one after attention, one after feedforward.\n",
    "\n",
    "## Forward Pass Logic\n",
    "1. **First Residual Block (Attention):**\n",
    "   - Save the input as `shortcut`.\n",
    "   - Apply layer normalization, then multi-head attention, then dropout.\n",
    "   - Add the original input (`shortcut`) to the output (residual connection).\n",
    "\n",
    "2. **Second Residual Block (FeedForward):**\n",
    "   - Save the new input as `shortcut`.\n",
    "   - Apply layer normalization, then the feedforward network, then dropout.\n",
    "   - Add the input (`shortcut`) to the output (residual connection).\n",
    "\n",
    "3. **Return the final output.**\n",
    "\n",
    "## Why This Structure?\n",
    "- **Residual connections** help gradients flow through deep networks, making it possible to train very deep transformer models.\n",
    "- **Layer normalization** before each sub-layer (pre-norm) improves stability and convergence, especially in large models.\n",
    "- **Multi-head attention** and **feedforward networks** are the two main sub-layers in each transformer block, enabling the model to learn both contextual and token-specific representations.\n",
    "\n",
    "## Usage in Transformers\n",
    "- This block is stacked multiple times in transformer architectures (e.g., 12, 24, or more layers).\n",
    "- The output of one block is the input to the next, allowing the model to build increasingly complex representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89401bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention.MultiHeadAttention import MultiHeadAttention\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            input_dim = config[\"embedding_dim\"],\n",
    "            output_dim = config[\"embedding_dim\"],\n",
    "            dropout = config[\"dropout\"],\n",
    "            context_length = config[\"context_length\"],\n",
    "            num_heads = config[\"num_heads\"],\n",
    "            qkv_bias = config[\"qkv_bias\"])\n",
    "        self.layer_norm1 = LayerNorm(config[\"embedding_dim\"])\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.layer_norm2 = LayerNorm(config[\"embedding_dim\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daca869",
   "metadata": {},
   "source": [
    "# Example: Running a Transformer Block on Input Data\n",
    " \n",
    "This markdown explains the demonstration code for instantiating and running the `TransformerBlock` on a batch of example input data.\n",
    "\n",
    "## What Does This Code Do?\n",
    "- **Sets a manual random seed** for reproducibility using `torch.manual_seed(246)`.\n",
    "- **Instantiates** the `TransformerBlock` class with the model configuration dictionary (`SYDSGPT_CONFIG_345M`).\n",
    "- **Creates** an example input tensor of shape `(2, 6, embedding_dim)`, representing a batch of 2 sequences, each with 6 tokens, and each token represented by a vector of size `embedding_dim`.\n",
    "- **Runs** the input through the transformer block to obtain the output tensor.\n",
    "- **Prints** the shape and values of both the input and output tensors for inspection.\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "1. **Random Seed Setup**\n",
    "   - `torch.manual_seed(246)` ensures that the random numbers generated (for the input tensor and model initialization) are reproducible.\n",
    "\n",
    "2. **Transformer Block Instantiation**\n",
    "   - `transformer = TransformerBlock(SYDSGPT_CONFIG_345M)` creates a transformer block using the specified configuration.\n",
    "\n",
    "3. **Example Input Creation**\n",
    "   - `example_input = torch.randn(2, 6, SYDSGPT_CONFIG_345M[\"embedding_dim\"])` generates a random tensor to simulate a batch of token embeddings.\n",
    "   - Shape: `(batch_size, sequence_length, embedding_dim)`.\n",
    "\n",
    "4. **Forward Pass**\n",
    "   - `output = transformer(example_input)` passes the input through the transformer block, applying multi-head attention, feedforward network, layer normalization, dropout, and residual connections.\n",
    "\n",
    "5. **Output Inspection**\n",
    "   - Prints the shape and values of the input and output tensors.\n",
    "   - The output shape matches the input: `(2, 6, embedding_dim)`, since the transformer block preserves the sequence and embedding dimensions.\n",
    "\n",
    "## Why Is This Important?\n",
    "- This example shows how to use a transformer block as part of a larger model, processing batches of token embeddings.\n",
    "- The transformer block is a key component in architectures like GPT and BERT, enabling the model to learn complex relationships and representations.\n",
    "- Understanding the input and output shapes is crucial for integrating transformer blocks into deep learning pipelines.\n",
    "\n",
    "## Usage in Practice\n",
    "- In a full transformer model, multiple transformer blocks are stacked to process input sequences, with each block building on the representations learned by the previous one.\n",
    "- The pattern of using random input data is common for testing and debugging model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da18b28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input Shape: torch.Size([2, 6, 1024])\n",
      "Transformer Block Output Shape: torch.Size([2, 6, 1024])\n",
      "Transformer Block Output: \n",
      " tensor([[[ 2.6021, -0.6464, -1.1488,  ..., -0.1816, -1.0399, -0.2072],\n",
      "         [-0.4377,  1.4974, -1.1613,  ...,  0.2095, -0.7198, -2.3146],\n",
      "         [ 1.7130,  0.8384,  0.0355,  ...,  0.4016, -0.5036, -1.5147],\n",
      "         [ 0.4436, -0.5751, -1.2977,  ..., -2.1431, -1.8006,  1.2133],\n",
      "         [ 0.9577,  1.0001, -2.2591,  ..., -1.0065,  0.3973,  0.6356],\n",
      "         [-0.2310, -0.9183,  0.0729,  ...,  0.7959, -0.8819,  0.5912]],\n",
      "\n",
      "        [[-1.4568, -1.9117, -0.2138,  ...,  0.5810, -1.6277, -2.8289],\n",
      "         [ 0.3885,  1.0216, -0.5874,  ..., -0.3243,  1.3307, -0.8727],\n",
      "         [-1.2116, -0.2976, -0.3767,  ...,  1.7585,  0.1871, -0.5062],\n",
      "         [ 0.0051, -1.5070, -1.0969,  ..., -0.2433, -0.8636,  0.0646],\n",
      "         [ 0.0226,  0.1599,  1.1637,  ..., -1.2578,  1.5422, -2.0616],\n",
      "         [ 1.1118,  1.3035, -0.0383,  ..., -1.6219, -1.5757, -2.3353]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Transformer Block Output Shape: torch.Size([2, 6, 1024])\n",
      "Transformer Block Output: \n",
      " tensor([[[ 2.6021, -0.6464, -1.1488,  ..., -0.1816, -1.0399, -0.2072],\n",
      "         [-0.4377,  1.4974, -1.1613,  ...,  0.2095, -0.7198, -2.3146],\n",
      "         [ 1.7130,  0.8384,  0.0355,  ...,  0.4016, -0.5036, -1.5147],\n",
      "         [ 0.4436, -0.5751, -1.2977,  ..., -2.1431, -1.8006,  1.2133],\n",
      "         [ 0.9577,  1.0001, -2.2591,  ..., -1.0065,  0.3973,  0.6356],\n",
      "         [-0.2310, -0.9183,  0.0729,  ...,  0.7959, -0.8819,  0.5912]],\n",
      "\n",
      "        [[-1.4568, -1.9117, -0.2138,  ...,  0.5810, -1.6277, -2.8289],\n",
      "         [ 0.3885,  1.0216, -0.5874,  ..., -0.3243,  1.3307, -0.8727],\n",
      "         [-1.2116, -0.2976, -0.3767,  ...,  1.7585,  0.1871, -0.5062],\n",
      "         [ 0.0051, -1.5070, -1.0969,  ..., -0.2433, -0.8636,  0.0646],\n",
      "         [ 0.0226,  0.1599,  1.1637,  ..., -1.2578,  1.5422, -2.0616],\n",
      "         [ 1.1118,  1.3035, -0.0383,  ..., -1.6219, -1.5757, -2.3353]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(246)\n",
    "transformer = TransformerBlock(SYDSGPT_CONFIG_345M)\n",
    "example_input = torch.randn(2, 6, SYDSGPT_CONFIG_345M[\"embedding_dim\"])\n",
    "output = transformer(example_input)\n",
    "print(f\"Example Input Shape: {example_input.shape}\")\n",
    "print(f\"Transformer Block Output Shape: {output.shape}\")\n",
    "print(f\"Transformer Block Output: \\n {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d98ccb",
   "metadata": {},
   "source": [
    "# SydsGPT Model: Full GPT-2 Style Architecture\n",
    " \n",
    "This markdown explains the implementation and design of the `SydsGPT` class, which represents a complete GPT-2 style transformer language model built using PyTorch. This class brings together all the core components defined earlier in the notebook, including embeddings, transformer blocks, normalization, and output projection.\n",
    "\n",
    "## Overview of the SydsGPT Class\n",
    "- **Purpose:** Implements a full transformer-based language model, similar to GPT-2, capable of processing sequences of tokenized text and generating predictions for the next token in a sequence.\n",
    "- **Structure:** The model is composed of several key submodules, each responsible for a specific part of the computation pipeline.\n",
    "\n",
    "## Components Explained\n",
    "1. **Token Embedding (`self.token_embedding`)**\n",
    "   - Maps each input token (integer ID) to a high-dimensional vector representation of size `embedding_dim`.\n",
    "   - Allows the model to learn semantic relationships between tokens.\n",
    "\n",
    "2. **Position Embedding (`self.position_embedding`)**\n",
    "   - Adds information about the position of each token in the sequence, enabling the model to distinguish between tokens at different positions.\n",
    "   - Essential for transformer models, which lack inherent sequence order awareness.\n",
    "\n",
    "3. **Dropout on Embeddings (`self.drop_embedding`)**\n",
    "   - Applies dropout to the sum of token and position embeddings, helping prevent overfitting during training.\n",
    "\n",
    "4. **Stacked Transformer Blocks (`self.transformer_blocks`)**\n",
    "   - A sequence of `num_layers` transformer blocks, each containing multi-head self-attention, feedforward networks, layer normalization, dropout, and residual connections.\n",
    "   - Each block refines the token representations by attending to other tokens and applying non-linear transformations.\n",
    "\n",
    "5. **Final Layer Normalization (`self.final_layer_norm`)**\n",
    "   - Normalizes the output of the last transformer block, stabilizing training and improving convergence.\n",
    "\n",
    "6. **Output Projection (`self.output_projection`)**\n",
    "   - Projects the final hidden states to the vocabulary size, producing logits for each token position.\n",
    "   - These logits are used to predict the next token in the sequence during training or inference.\n",
    "\n",
    "## Forward Pass Logic\n",
    "1. **Input:**\n",
    "   - Expects an input tensor of shape `(batch_size, sequence_length)`, where each element is a token ID.\n",
    "2. **Embedding:**\n",
    "   - Computes token and position embeddings, sums them, and applies dropout.\n",
    "3. **Transformer Blocks:**\n",
    "   - Passes the embeddings through the stack of transformer blocks, allowing the model to learn complex dependencies and contextual representations.\n",
    "4. **Normalization and Output:**\n",
    "   - Applies final layer normalization and projects the result to the vocabulary size, producing logits for each token position.\n",
    "5. **Return:**\n",
    "   - Returns the logits tensor of shape `(batch_size, sequence_length, vocab_size)`.\n",
    "\n",
    "## Why This Structure?\n",
    "- **Scalability:** The modular design allows for easy scaling by adjusting the number of layers, embedding size, and other hyperparameters.\n",
    "- **Expressiveness:** Stacking multiple transformer blocks enables the model to capture deep, hierarchical relationships in text.\n",
    "- **Standard Practice:** This architecture closely follows the design of GPT-2 and similar large language models, making it suitable for a wide range of NLP tasks.\n",
    "\n",
    "## Usage in Practice\n",
    "- The `SydsGPT` class can be instantiated with a configuration dictionary and used for training or inference on tokenized text data.\n",
    "- During training, the model's output logits are typically passed to a loss function (e.g., cross-entropy) to optimize the model parameters.\n",
    "- For text generation, the model can be used in an autoregressive fashion, predicting one token at a time based on previous context.\n",
    "\n",
    "## Summary Table of Key Components\n",
    "| Component              | Purpose                                              | Output Shape                        |\n",
    "|------------------------|------------------------------------------------------|-------------------------------------|\n",
    "| Token Embedding        | Learnable token representations                      | (batch_size, seq_length, emb_dim)   |\n",
    "| Position Embedding     | Learnable position information                       | (batch_size, seq_length, emb_dim)   |\n",
    "| Dropout                | Regularization on embeddings                         | (batch_size, seq_length, emb_dim)   |\n",
    "| Transformer Blocks     | Contextualize token representations                  | (batch_size, seq_length, emb_dim)   |\n",
    "| Final Layer Norm       | Normalize final hidden states                        | (batch_size, seq_length, emb_dim)   |\n",
    "| Output Projection      | Project to vocabulary logits                         | (batch_size, seq_length, vocab_size)|\n",
    "\n",
    "This class serves as the backbone of a GPT-2 style language model, integrating all the essential components for modern transformer-based NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97bcb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SydsGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"embedding_dim\"])\n",
    "        self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"embedding_dim\"])\n",
    "        self.drop_embedding = nn.Dropout(config[\"dropout\"])\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"num_layers\"])])\n",
    "        self.final_layer_norm = LayerNorm(config[\"embedding_dim\"])\n",
    "        self.output_projection = nn.Linear(config[\"embedding_dim\"], config[\"vocab_size\"], bias = False)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        batch_size, seq_length = input.shape\n",
    "        token_embeddings = self.token_embedding(input)\n",
    "        position_embeddings = self.position_embedding(torch.arange(seq_length, device=input.device))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.drop_embedding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c55a3",
   "metadata": {},
   "source": [
    "# Example: Running the Full SydsGPT Model on Tokenized Input\n",
    " \n",
    "This markdown explains the demonstration code for instantiating and running the full `SydsGPT` model on a batch of tokenized input data. This example shows how to use the complete transformer-based language model defined in the previous cell.\n",
    "\n",
    "## What Does This Code Do?\n",
    "- **Sets a manual random seed** for reproducibility using `torch.manual_seed(246)`.\n",
    "- **Instantiates** the `SydsGPT` class with the model configuration dictionary (`SYDSGPT_CONFIG_345M`).\n",
    "- **Runs** a batch of tokenized input data (created earlier in the notebook) through the model to obtain output logits.\n",
    "- **Prints** the input batch, the shape of the logits tensor, and the logits themselves for inspection.\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "1. **Random Seed Setup**\n",
    "   - `torch.manual_seed(246)` ensures that the random numbers generated for model initialization are reproducible.\n",
    "\n",
    "2. **Model Instantiation**\n",
    "   - `sydsgpt_model = SydsGPT(SYDSGPT_CONFIG_345M)` creates an instance of the full transformer model using the specified configuration.\n",
    "\n",
    "3. **Forward Pass**\n",
    "   - `logits = sydsgpt_model(batch)` passes the tokenized input batch through the model.\n",
    "   - The model processes the input through embeddings, stacked transformer blocks, normalization, and output projection to produce logits for each token position.\n",
    "\n",
    "4. **Output Inspection**\n",
    "   - Prints the input batch tensor for reference.\n",
    "   - Prints the shape of the logits tensor, which should be `(batch_size, sequence_length, vocab_size)`.\n",
    "   - Prints the logits tensor itself, which contains the unnormalized predictions for each token position in the input.\n",
    "\n",
    "## Why Is This Important?\n",
    "- This example demonstrates how to use the full transformer model for inference or training on tokenized text data.\n",
    "- The output logits can be used to compute loss during training (e.g., with cross-entropy) or to generate text by sampling or selecting the most likely next token.\n",
    "- Understanding the input and output shapes is crucial for integrating the model into larger NLP pipelines.\n",
    "\n",
    "## Usage in Practice\n",
    "- In a real-world scenario, the input batch would be created by tokenizing raw text using a tokenizer (as shown earlier in the notebook).\n",
    "- The model can be trained on large text corpora to learn language patterns, or used for downstream tasks such as text generation, completion, or classification.\n",
    "- The logits output by the model are typically passed to a softmax function to obtain probabilities over the vocabulary for each token position.\n",
    "\n",
    "This cell provides a practical example of running a complete GPT-2 style model on tokenized input, serving as a template for further experimentation and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e539101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[15496,   703,   389,   345],\n",
      "        [ 2061,   389,   345,  1804]])\n",
      "Logits Shape: torch.Size([2, 4, 50257])\n",
      "Logits: tensor([[[ 0.1230,  0.3697, -0.4113,  ..., -0.0493, -0.4047, -0.3231],\n",
      "         [ 0.3657, -0.0389, -0.2145,  ..., -0.2732, -0.2118, -0.7021],\n",
      "         [ 0.5050, -0.6050,  0.2186,  ..., -0.7836, -0.2520, -0.2040],\n",
      "         [ 0.3738,  0.2956,  0.5254,  ...,  1.0762, -0.7500, -0.0704]],\n",
      "\n",
      "        [[ 0.5839, -0.2989, -0.1086,  ..., -0.3229, -0.1380, -0.2090],\n",
      "         [-0.0806,  0.2465, -0.0125,  ..., -1.4313, -0.6350, -0.6414],\n",
      "         [-0.5175, -0.8065,  0.2659,  ...,  0.6558, -0.1612, -0.0930],\n",
      "         [ 0.5556, -0.1440,  0.0022,  ...,  1.2245, -0.1364, -0.6378]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(246)\n",
    "sydsgpt_model = SydsGPT(SYDSGPT_CONFIG_345M)\n",
    "logits = sydsgpt_model(batch)\n",
    "print(f\"Input: {batch}\")\n",
    "print(f\"Logits Shape: {logits.shape}\")\n",
    "print(f\"Logits: {logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1567f",
   "metadata": {},
   "source": [
    "# Calculating Total Parameters in the SydsGPT Model\n",
    " \n",
    "This markdown explains the code cell that calculates and prints the total number of trainable parameters in the `SydsGPT` model. Understanding the parameter count is crucial for assessing the model's size, memory requirements, and computational complexity.\n",
    "\n",
    "## What Does This Code Do?\n",
    "- **Computes the total number of parameters** in the `SydsGPT` model by summing the number of elements in each parameter tensor.\n",
    "- **Prints** the total parameter count, providing insight into the model's scale (e.g., whether it matches the intended \"345M\" parameter target).\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "1. **Parameter Counting**\n",
    "   - `sydsgpt_model.parameters()` returns an iterator over all trainable parameters (weights and biases) in the model.\n",
    "   - For each parameter tensor, `parameter.numel()` returns the total number of elements (i.e., the number of scalars in that tensor).\n",
    "   - The `sum(...)` function adds up the element counts for all parameters, yielding the total number of trainable parameters in the model.\n",
    "\n",
    "2. **Printing the Result**\n",
    "   - The total parameter count is printed in a human-readable format.\n",
    "\n",
    "## Why Is This Important?\n",
    "- **Model Size:** The number of parameters directly determines the model's memory footprint and computational requirements.\n",
    "- **Benchmarking:** Comparing parameter counts helps benchmark against other models (e.g., GPT-2 small, medium, large, XL).\n",
    "- **Resource Planning:** Knowing the parameter count is essential for planning training hardware (GPUs/TPUs), estimating training time, and managing deployment constraints.\n",
    "- **Sanity Check:** Ensures that the model architecture matches the intended design (e.g., \"345M\" parameters for a GPT-2 medium-sized model).\n",
    "\n",
    "## Usage in Practice\n",
    "- This code can be used after defining or modifying a model to quickly check its size.\n",
    "- For large models, parameter counts are often reported in millions (M) or billions (B). For example, a value of `345,000,000` means the model has 345 million parameters.\n",
    "- The parameter count should be consistent with the model's configuration (number of layers, embedding size, etc.).\n",
    "\n",
    "This cell provides a simple but essential check for anyone developing or experimenting with deep learning models, especially large-scale transformer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08c1a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in SydsGPT Model: 406212608\n"
     ]
    }
   ],
   "source": [
    "total_parameters = sum(parameter.numel() for parameter in sydsgpt_model.parameters())\n",
    "print(f\"Total Parameters in SydsGPT Model: {total_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0b980",
   "metadata": {},
   "source": [
    "# Estimating the SydsGPT Model's Memory Footprint\n",
    " \n",
    "This markdown explains the code cell that estimates and prints the total memory size of the `SydsGPT` model in megabytes (MB). Understanding the memory requirements is essential for planning training, deployment, and hardware selection for large neural networks.\n",
    "\n",
    "## What Does This Code Do?\n",
    "- **Calculates the total memory size** of all model parameters, assuming each parameter is stored as a 32-bit (4-byte) floating-point number (the default in PyTorch).\n",
    "- **Prints** the total model size in megabytes (MB), providing a practical sense of the model's memory footprint.\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "1. **Parameter Size Calculation**\n",
    "   - `total_parameters` is the total number of trainable parameters in the model (computed in the previous cell).\n",
    "   - Each parameter is assumed to be a 32-bit float, which occupies 4 bytes of memory.\n",
    "   - `total_size_bytes = total_parameters * 4` computes the total size in bytes.\n",
    "\n",
    "2. **Conversion to Megabytes**\n",
    "   - `total_size_mb = total_size_bytes / (1024 ** 2)` converts the size from bytes to megabytes (1 MB = 1,048,576 bytes).\n",
    "   - This makes the result more interpretable and easier to compare with available system or GPU memory.\n",
    "\n",
    "3. **Printing the Result**\n",
    "   - The total model size is printed with two decimal places for clarity.\n",
    "\n",
    "## Why Is This Important?\n",
    "- **Hardware Planning:** Knowing the model's memory footprint is crucial for selecting appropriate hardware (e.g., GPU/TPU with sufficient VRAM).\n",
    "- **Training Feasibility:** Large models may not fit into memory on smaller devices, requiring model parallelism, gradient checkpointing, or other memory-saving techniques.\n",
    "- **Deployment:** Understanding memory requirements helps when deploying models to production environments, especially for edge or mobile devices.\n",
    "- **Optimization:** Memory usage can be a bottleneck; knowing the size helps guide optimization efforts (e.g., quantization, pruning, mixed precision).\n",
    "\n",
    "## Usage in Practice\n",
    "- This calculation assumes all parameters are stored as 32-bit floats. If using mixed precision (e.g., float16), the memory footprint would be roughly halved.\n",
    "- The reported size does not include additional memory used for activations, gradients, optimizer states, or temporary buffers during training or inference.\n",
    "- For very large models, memory requirements can quickly exceed the capacity of a single device, necessitating distributed or sharded training.\n",
    "\n",
    "This cell provides a quick and practical estimate of the model's memory requirements, which is essential for anyone working with large-scale deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37754379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Size: 1549.58 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_parameters * 4\n",
    "total_size_mb = total_size_bytes / (1024 ** 2)\n",
    "print(f\"Total Model Size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dd25a",
   "metadata": {},
   "source": [
    "# Simple Greedy Text Generation Function for SydsGPT\n",
    " \n",
    "This markdown explains the code cell that defines `generate_simple`, a basic text generation function for the `SydsGPT` model. This function demonstrates how to use a trained language model to generate sequences of tokens in an autoregressive (one token at a time) fashion.\n",
    "\n",
    "## What Does This Code Do?\n",
    "- **Implements a simple greedy decoding loop** for generating text from a transformer-based language model.\n",
    "- **Takes an initial sequence of token IDs** and repeatedly predicts and appends the most likely next token, up to a specified maximum length.\n",
    "- **Handles context windowing** to ensure the model only sees the most recent tokens up to its maximum context size.\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "1. **Function Signature**\n",
    "   - `generate_simple(model, input_ids, max_length, context_size)`\n",
    "     - `model`: The language model (e.g., `SydsGPT`) to use for generation.\n",
    "     - `input_ids`: A tensor of shape `(batch_size, current_sequence_length)` containing the initial token IDs for each sequence in the batch.\n",
    "     - `max_length`: The number of new tokens to generate.\n",
    "     - `context_size`: The maximum number of tokens the model can attend to (should match the model's context window, e.g., 1024).\n",
    "\n",
    "2. **Generation Loop**\n",
    "   - For each step up to `max_length`:\n",
    "     - **Context Cropping:**\n",
    "       - `input_ids_crop = input_ids[:, -context_size:]` ensures that only the most recent `context_size` tokens are fed to the model, respecting the model's context window.\n",
    "     - **Model Forward Pass:**\n",
    "       - `logits = model(input_ids_crop)` computes the output logits for the current context. `torch.no_grad()` disables gradient computation for efficiency.\n",
    "     - **Next Token Selection:**\n",
    "       - `next_token_logits = logits[:, -1, :]` extracts the logits for the last position in the sequence (the next token to predict).\n",
    "       - `next_token_probs = torch.softmax(next_token_logits, dim = -1)` converts logits to probabilities over the vocabulary.\n",
    "       - `next_token = torch.argmax(next_token_probs, dim = -1, keepdim = True)` selects the most likely next token (greedy decoding).\n",
    "     - **Sequence Extension:**\n",
    "       - `input_ids = torch.cat((input_ids, next_token), dim = 1)` appends the predicted token to the sequence for the next iteration.\n",
    "\n",
    "3. **Return Value**\n",
    "   - Returns the final tensor of token IDs, now extended by `max_length` new tokens for each sequence in the batch.\n",
    "\n",
    "## Why Is This Important?\n",
    "- **Autoregressive Generation:** This function demonstrates the core principle of autoregressive text generation, where each new token is predicted based on all previous tokens.\n",
    "- **Greedy Decoding:** The function uses greedy decoding (always picking the most likely token), which is simple but may not produce the most diverse or creative outputs. More advanced methods include sampling, top-k, or nucleus (top-p) sampling.\n",
    "- **Context Management:** Properly handling the context window is essential for large models, which may have strict limits on the number of tokens they can process at once.\n",
    "- **Practical Usage:** This function can be used to generate text completions, simulate dialogue, or test the model's ability to extend sequences.\n",
    "\n",
    "## Usage in Practice\n",
    "- To use this function, provide a trained model, an initial sequence of token IDs, the number of tokens to generate, and the model's context size.\n",
    "- The output can be decoded back to text using the tokenizer's `decode` method.\n",
    "- For more realistic or creative text, consider implementing sampling-based decoding strategies.\n",
    "\n",
    "This cell provides a clear and practical example of how to perform basic text generation with a transformer-based language model in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd110125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple(model, input_ids, max_length, context_size):\n",
    "    for _ in range(max_length):\n",
    "        input_ids_crop = input_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits  = model(input_ids_crop)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim = -1)\n",
    "        next_token = torch.argmax(next_token_probs, dim = -1, keepdim = True)\n",
    "        input_ids = torch.cat((input_ids, next_token), dim = 1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d444b9a",
   "metadata": {},
   "source": [
    "# Text Generation from a Prompt: End-to-End Example\n",
    "\n",
    "This cell shows how to generate text with the `SydsGPT` model using the `generate_simple` function and a plain-English prompt.\n",
    "\n",
    "- Prompt and tokenization:\n",
    "  - `start_context = \"Once upon a time\"` is the starting text.\n",
    "  - `tokenizer.encode(...)` converts the prompt into token IDs.\n",
    "  - `torch.tensor(...).unsqueeze(0)` adds a batch dimension so the input shape is `(batch_size=1, seq_len)`.\n",
    "\n",
    "- Model eval mode:\n",
    "  - `sydsgpt_model.eval()` disables dropout and gradients for deterministic, faster inference.\n",
    "\n",
    "- Generation loop (greedy decoding):\n",
    "  - `context_size = SYDSGPT_CONFIG_345M[\"context_length\"]` sets the max number of tokens the model attends to.\n",
    "  - `generate_simple(model, input_ids, 10, context_size)` appends 10 new tokens by repeatedly:\n",
    "    1) Cropping to the last `context_size` tokens,\n",
    "    2) Running the model to get logits,\n",
    "    3) Taking the argmax token (most probable next token),\n",
    "    4) Concatenating it to the sequence.\n",
    "\n",
    "- Decoding:\n",
    "  - `tokenizer.decode(...)` converts the generated token IDs back to readable text.\n",
    "\n",
    "Notes and tips:\n",
    "- Greedy decoding is simple and fast, but can be repetitive. For more variety, consider top-k or nucleus (top-p) sampling.\n",
    "- If your prompt is long, the function automatically crops to the most recent `context_size` tokens to respect the model’s context window.\n",
    "- To generate longer outputs, increase the `max_length` argument in `generate_simple`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc54ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Context: [7454, 2402, 257, 640]\n",
      "Input IDs Shape: torch.Size([1, 4])\n",
      "Generated IDs: tensor([[ 7454,  2402,   257,   640, 22021,  7857, 44144, 45836,  4328, 41871,\n",
      "         43984, 32880, 11958, 21420]])\n",
      "Generated Text: Once upon a time theirssize Sharif grunt splrfBangflex penet Nest\n",
      "Generated IDs: tensor([[ 7454,  2402,   257,   640, 22021,  7857, 44144, 45836,  4328, 41871,\n",
      "         43984, 32880, 11958, 21420]])\n",
      "Generated Text: Once upon a time theirssize Sharif grunt splrfBangflex penet Nest\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Once upon a time\"\n",
    "encoded_context = tokenizer.encode(start_context)\n",
    "input_ids = torch.tensor(encoded_context).unsqueeze(0)\n",
    "print(f\"Encoded Context: {encoded_context}\")\n",
    "print(f\"Input IDs Shape: {input_ids.shape}\")\n",
    "\n",
    "sydsgpt_model.eval()\n",
    "\n",
    "context_size = SYDSGPT_CONFIG_345M[\"context_length\"]\n",
    "generated_ids = generate_simple(sydsgpt_model, input_ids, 10, context_size)\n",
    "print(f\"Generated IDs: {generated_ids}\")\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids.squeeze(0).tolist())\n",
    "print(f\"Generated Text: {generated_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
