{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364e9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec6ca1",
   "metadata": {},
   "source": [
    "# SYD'S GPT-345M Model Configuration\n",
    "\n",
    "This notebook cell defines the configuration dictionary for a GPT-style language model with approximately 345 million parameters. The configuration parameters are as follows:\n",
    "\n",
    "- **vocab_size**: The number of unique tokens (words, subwords, or characters) that the model can understand. For GPT-345M, this is set to 50,257, matching the standard GPT-2 vocabulary size.\n",
    "- **context_length**: The maximum number of tokens the model can consider in a single input sequence. Here, it is set to 1,024 tokens, which determines the model's memory window for generating or analyzing text.\n",
    "- **embedding_dim**: The size of the vector used to represent each token. A higher embedding dimension allows the model to capture more nuanced relationships between tokens. This configuration uses 1,024 dimensions.\n",
    "- **num_heads**: The number of attention heads in the multi-head self-attention mechanism. More heads allow the model to focus on different parts of the input simultaneously. This model uses 16 heads.\n",
    "- **num_layers**: The number of transformer blocks (layers) stacked in the model. More layers generally increase the model's capacity to learn complex patterns. Here, 24 layers are used.\n",
    "- **dropout**: The dropout rate applied during training to prevent overfitting. A value of 0.1 means 10% of the connections are randomly dropped during each training step.\n",
    "- **qkv_bias**: A boolean indicating whether to include a bias term in the query, key, and value projections of the attention mechanism. Setting this to `False` means no bias is added.\n",
    "\n",
    "This configuration is typical for a medium-sized GPT-2 model and can be used as a starting point for training or fine-tuning a transformer-based language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddaabe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYDSGPT_CONFIG_345M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"embedding_dim\" : 1024,\n",
    "    \"num_heads\" : 16,\n",
    "    \"num_layers\" : 24,\n",
    "    \"dropout\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed747498",
   "metadata": {},
   "source": [
    "# PlaceholderGPT Model Class Overview\n",
    "\n",
    "The following code cell defines a simplified, illustrative version of a GPT-style transformer model using PyTorch. This implementation is intended for educational purposes and demonstrates the core architectural components of a transformer-based language model:\n",
    "\n",
    "## Main Components\n",
    "\n",
    "- **PlaceholderGPT (nn.Module)**: The main model class. It initializes the following layers:\n",
    "  - **Token Embedding**: Maps each token in the input sequence to a high-dimensional vector.\n",
    "  - **Position Embedding**: Adds positional information to each token, allowing the model to distinguish between tokens at different positions.\n",
    "  - **Dropout Layer**: Regularizes the model by randomly dropping units during training to prevent overfitting.\n",
    "  - **Transformer Blocks**: A stack of placeholder transformer blocks (not fully implemented here) that would normally perform self-attention and feed-forward operations.\n",
    "  - **Final Layer Normalization**: Normalizes the output of the transformer blocks.\n",
    "  - **Output Projection**: Projects the final hidden states to the vocabulary size, producing logits for each token.\n",
    "\n",
    "- **forward(input)**: The forward pass of the model. It processes the input sequence through embeddings, dropout, transformer blocks, layer normalization, and output projection to produce logits for each token position.\n",
    "\n",
    "## Supporting Classes\n",
    "\n",
    "- **PlaceholderTransformerBlock (nn.Module)**: Represents a single transformer block. In a full implementation, this would include multi-head self-attention and feed-forward layers. Here, it simply returns the input unchanged.\n",
    "\n",
    "- **PlaceholderLayerNorm (nn.Module)**: A placeholder for layer normalization. In a real model, this would normalize the input tensor across the embedding dimension.\n",
    "\n",
    "### What does `eps = 1e-5` mean?\n",
    "\n",
    "In the context of neural networks and layer normalization, `eps` (epsilon) is a small constant added to the denominator when normalizing activations. This prevents division by zero and improves numerical stability.\n",
    "\n",
    "- **Value:** `1e-5` means epsilon is set to $0.00001$.\n",
    "- **Purpose:** When calculating the normalized output, the formula typically looks like:\n",
    "  $$\n",
    "  \\text{normalized} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "  $$\n",
    "  where $\\mu$ is the mean and $\\sigma^2$ is the variance of the input $x$.\n",
    "- **Why needed:** If the variance is very close to zero, adding epsilon ensures the denominator is never zero, avoiding undefined results and improving stability during training.\n",
    "\n",
    "This is a standard practice in deep learning layers such as batch normalization and layer normalization.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This code is a skeleton and does not implement the full transformer logic (e.g., attention mechanisms, feed-forward networks, or actual layer normalization).\n",
    "- The model uses configuration parameters such as `vocab_size`, `embedding_dim`, `context_length`, `num_layers`, and `dropout` from the configuration dictionary defined earlier in the notebook.\n",
    "- The purpose of this cell is to illustrate the structure and flow of a transformer-based language model in PyTorch, serving as a starting point for further development or experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d152a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceholderGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"embedding_dim\"])\n",
    "        self.position_embedding = nn.Embedding(config[\"context_length\"], config[\"embedding_dim\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        self.transformer_blocks = nn.Sequential(*[PlaceholderTransformerBlock(config) for _ in range(config[\"num_layers\"])])\n",
    "        self.final_layer_norm = PlaceholderLayerNorm(config[\"embedding_dim\"])\n",
    "        self.output_projection = nn.Linear(config[\"embedding_dim\"], config[\"vocab_size\"], bias = False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_length = input.shape\n",
    "        token_embeddings = self.token_embedding(input)\n",
    "        position_embeddings = self.position_embedding(torch.arange(seq_length, device = input.device))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        return logits\n",
    "    \n",
    "class PlaceholderTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class PlaceholderLayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b9463",
   "metadata": {},
   "source": [
    "# Example: Tokenization and Model Forward Pass\n",
    "\n",
    "This code cell demonstrates how to tokenize input text, prepare a batch for the model, and run a forward pass using the `PlaceholderGPT` model defined earlier in the notebook.\n",
    "\n",
    "## Steps Explained\n",
    "\n",
    "1. **Tokenization**\n",
    "   - Uses the `tiktoken` library to obtain the GPT-2 tokenizer.\n",
    "   - Two example sentences (`ex1` and `ex2`) are encoded into lists of token IDs.\n",
    "\n",
    "2. **Batch Preparation**\n",
    "   - The encoded token lists are converted to PyTorch tensors and added to a batch list.\n",
    "   - The batch is stacked into a single tensor with shape `(batch_size, sequence_length)`.\n",
    "   - The batch tensor is printed to show its contents.\n",
    "\n",
    "3. **Model Initialization and Forward Pass**\n",
    "   - Sets a manual random seed for reproducibility.\n",
    "   - Instantiates the `PlaceholderGPT` model with the configuration dictionary.\n",
    "   - Runs the batch through the model to obtain logits (unnormalized predictions for each token position).\n",
    "\n",
    "4. **Output**\n",
    "   - Prints the shape of the logits tensor, which should be `(batch_size, sequence_length, vocab_size)`.\n",
    "   - Prints the actual logits tensor values.\n",
    "\n",
    "## Notes\n",
    "- This example uses a placeholder model, so the logits are not meaningful predictions but demonstrate the expected output structure.\n",
    "- The code illustrates the typical workflow for preparing text data and running it through a transformer-based language model in PyTorch.\n",
    "- The use of `torch.manual_seed` ensures that results are reproducible for debugging and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b808240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,   703,   389,   345],\n",
      "        [ 2061,   389,   345,  1804]])\n",
      "Logits Shape: torch.Size([2, 4, 50257])\n",
      "Logits: \n",
      " tensor([[[ 0.2638, -0.1249, -1.9838,  ..., -0.0289,  0.6842, -0.1247],\n",
      "         [-0.0896,  1.0701,  0.0236,  ...,  0.4069, -0.5886, -0.4666],\n",
      "         [ 0.6501,  0.0914, -1.0634,  ...,  1.0771,  0.1660, -0.0224],\n",
      "         [ 0.5803,  0.3458, -0.4553,  ...,  0.7233, -1.3835, -0.2096]],\n",
      "\n",
      "        [[ 1.6676,  0.5273, -0.3480,  ..., -0.0478, -0.2177,  0.0361],\n",
      "         [ 0.6377, -0.6156, -1.3517,  ...,  1.0306, -1.0850, -1.3458],\n",
      "         [ 0.1607,  0.2084,  0.6130,  ...,  0.6961, -0.9398,  0.6000],\n",
      "         [-0.1086, -0.2255, -0.4622,  ...,  0.8606,  0.1524, -1.0448]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Logits Shape: torch.Size([2, 4, 50257])\n",
      "Logits: \n",
      " tensor([[[ 0.2638, -0.1249, -1.9838,  ..., -0.0289,  0.6842, -0.1247],\n",
      "         [-0.0896,  1.0701,  0.0236,  ...,  0.4069, -0.5886, -0.4666],\n",
      "         [ 0.6501,  0.0914, -1.0634,  ...,  1.0771,  0.1660, -0.0224],\n",
      "         [ 0.5803,  0.3458, -0.4553,  ...,  0.7233, -1.3835, -0.2096]],\n",
      "\n",
      "        [[ 1.6676,  0.5273, -0.3480,  ..., -0.0478, -0.2177,  0.0361],\n",
      "         [ 0.6377, -0.6156, -1.3517,  ...,  1.0306, -1.0850, -1.3458],\n",
      "         [ 0.1607,  0.2084,  0.6130,  ...,  0.6961, -0.9398,  0.6000],\n",
      "         [-0.1086, -0.2255, -0.4622,  ...,  0.8606,  0.1524, -1.0448]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "ex1 = \"Hello how are you\"\n",
    "ex2 = \"What are you doing\"\n",
    "batch.append(torch.tensor(tokenizer.encode(ex1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(ex2)))\n",
    "batch = torch.stack(batch, dim = 0)\n",
    "print(batch)\n",
    "\n",
    "torch.manual_seed(246)\n",
    "test_model = PlaceholderGPT(SYDSGPT_CONFIG_345M)\n",
    "logits = test_model(batch)\n",
    "print(f\"Logits Shape: {logits.shape}\")\n",
    "print(f\"Logits: \\n {logits}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f087c",
   "metadata": {},
   "source": [
    "# Example: Manual Layer Normalization in PyTorch\n",
    "\n",
    "This code cell demonstrates how to manually normalize the output of a neural network layer in PyTorch, illustrating the concept behind layer normalization.\n",
    "\n",
    "## Steps Explained\n",
    "\n",
    "1. **Batch and Layer Setup**\n",
    "   - Sets a manual random seed for reproducibility.\n",
    "   - Creates a random batch tensor `ex_batch` of shape `(2, 6)`.\n",
    "   - Defines a simple neural network layer (`nn_layer`) consisting of a linear transformation followed by a ReLU activation.\n",
    "   - Passes the batch through the layer to obtain `output`.\n",
    "   - Prints the shape and values of the output tensor.\n",
    "\n",
    "2. **Mean and Variance Calculation**\n",
    "   - Computes the mean and variance of the output tensor along the last dimension (features) for each sample in the batch.\n",
    "   - Prints the mean and variance values.\n",
    "\n",
    "3. **Manual Normalization**\n",
    "   - Normalizes the output tensor by subtracting the mean and dividing by the square root of the variance (without epsilon for simplicity).\n",
    "   - Prints the normalized output tensor.\n",
    "   - Calculates and prints the mean and variance of the normalized output to verify that the mean is close to zero and the variance is close to one for each sample.\n",
    "\n",
    "## Notes\n",
    "- This example shows the core idea behind layer normalization, which is commonly used in deep learning models to stabilize and accelerate training.\n",
    "- In practice, a small epsilon value is added to the denominator to prevent division by zero and improve numerical stability.\n",
    "- The normalization is performed per sample (row) in the batch, across the feature dimension.\n",
    "- This manual approach helps illustrate what built-in PyTorch layers like `nn.LayerNorm` do internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8ce5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([2, 8])\n",
      "Output: \n",
      " tensor([[0.0000, 1.4358, 0.8909, 0.0000, 0.3553, 1.4952, 0.0000, 0.0000],\n",
      "        [1.4236, 0.0000, 0.0000, 2.0742, 1.7022, 0.1547, 0.0589, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean: \n",
      " tensor([[0.5221],\n",
      "        [0.6767]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[0.4337],\n",
      "        [0.7986]], grad_fn=<VarBackward0>)\n",
      "Normalized Output: \n",
      " tensor([[-0.7929,  1.3874,  0.5599, -0.7929, -0.2533,  1.4775, -0.7929, -0.7929],\n",
      "        [ 0.8357, -0.7572, -0.7572,  1.5638,  1.1475, -0.5841, -0.6913, -0.7572]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean After Norm: \n",
      " tensor([[0.0000e+00],\n",
      "        [2.2352e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance After Norm: \n",
      " tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(246)\n",
    "ex_batch = torch.randn(2,6)\n",
    "nn_layer = nn.Sequential(nn.Linear(6,8), nn.ReLU())\n",
    "output = nn_layer(ex_batch)\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Output: \\n {output}\")\n",
    "\n",
    "mean = output.mean(dim = -1, keepdim = True)\n",
    "variance = output.var(dim = -1, keepdim = True)\n",
    "print(f\"Mean: \\n {mean}\")\n",
    "print(f\"Variance: \\n {variance}\")\n",
    "\n",
    "normalized_output = (output - mean) / torch.sqrt(variance)\n",
    "mean_after_norm = normalized_output.mean(dim = -1, keepdim = True)\n",
    "variance_after_norm = normalized_output.var(dim = -1, keepdim = True)\n",
    "print(f\"Normalized Output: \\n {normalized_output}\")\n",
    "print(f\"Mean After Norm: \\n {mean_after_norm}\")\n",
    "print(f\"Variance After Norm: \\n {variance_after_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbd1e0",
   "metadata": {},
   "source": [
    "# Improved Custom LayerNorm Class in PyTorch\n",
    "\n",
    "This code cell presents an improved custom implementation of the Layer Normalization operation, a key technique for stabilizing and accelerating training in deep learning models, especially transformers.\n",
    "\n",
    "## How This Implementation Works\n",
    "\n",
    "- **Class Definition:**\n",
    "  - `LayerNorm` inherits from `nn.Module` for seamless integration with PyTorch models.\n",
    "  - The constructor accepts `embedding_dim` (the size of the last dimension to normalize) and `eps` (a small constant for numerical stability).\n",
    "\n",
    "- **Parameters:**\n",
    "  - `scale`: A learnable scaling parameter (initialized to ones) that allows the model to adjust the normalized output's scale.\n",
    "  - `shift`: A learnable shifting parameter (initialized to zeros) that allows the model to adjust the normalized output's mean.\n",
    "\n",
    "- **Forward Pass:**\n",
    "  - Calculates the mean and variance of the input tensor `x` along the last dimension for each sample.\n",
    "  - Normalizes the input: $\\text{normalized}_x = \\frac{x - \\text{mean}}{\\sqrt{\\text{variance} + \\epsilon}}$\n",
    "  - Applies the learnable scale (`scale`) and shift (`shift`) to the normalized tensor.\n",
    "  - Returns the final output.\n",
    "\n",
    "## Key Points\n",
    "- Layer normalization is performed per sample, across the feature dimension, making it suitable for variable-length sequences and transformer models.\n",
    "- The use of `eps` ensures numerical stability by preventing division by zero.\n",
    "- Learnable parameters (`scale` and `shift`) allow the model to recover the original distribution if needed.\n",
    "- This implementation closely mirrors PyTorch's built-in `nn.LayerNorm`, but is written from scratch for educational clarity and flexibility.\n",
    "\n",
    "## Usage\n",
    "This custom `LayerNorm` class can be used in place of PyTorch's built-in layer normalization in neural network modules, especially when you want to understand, customize, or experiment with normalization behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19b6d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(embedding_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embedding_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        variance = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        normalized_x = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "        return self.scale * normalized_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65028f",
   "metadata": {},
   "source": [
    "# Using the Custom LayerNorm Class\n",
    "\n",
    "This code cell demonstrates how to use the custom `LayerNorm` class defined above to normalize a batch of data in PyTorch.\n",
    "\n",
    "## Steps Explained\n",
    "\n",
    "1. **LayerNorm Instantiation**\n",
    "   - Creates an instance of the custom `LayerNorm` class with `embedding_dim = 6`, matching the feature dimension of the input batch (`ex_batch`).\n",
    "\n",
    "2. **Applying Layer Normalization**\n",
    "   - Passes the batch tensor `ex_batch` through the `LayerNorm` instance to obtain `normalized_output`.\n",
    "   - Prints the normalized output tensor.\n",
    "\n",
    "3. **Verifying Normalization**\n",
    "   - Calculates and prints the mean and variance of the normalized output along the last dimension for each sample in the batch.\n",
    "   - The mean should be close to zero and the variance close to one, confirming that the normalization is working as intended (modulo learnable parameters, which are initialized to scale=1 and shift=0).\n",
    "\n",
    "## Notes\n",
    "- This example shows how to use a custom normalization layer in practice, similar to how you would use PyTorch's built-in `nn.LayerNorm`.\n",
    "- Layer normalization is applied independently to each sample (row) in the batch, across the feature dimension.\n",
    "- The learnable parameters (`scale` and `shift`) allow the model to adapt the normalized output during training.\n",
    "- This approach is essential in transformer models and other deep learning architectures to ensure stable and efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32040b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Norm Output: \n",
      " tensor([[-1.3450,  1.1168, -0.1748, -0.5987, -0.5124,  1.5140],\n",
      "        [-1.2331,  0.8340,  0.8506,  1.1080, -0.2246, -1.3349]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Mean After Layer Norm: \n",
      " tensor([[-3.9736e-08],\n",
      "        [-1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance After Layer Norm: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = LayerNorm(embedding_dim = 6)\n",
    "normalized_output = layer_norm(ex_batch)\n",
    "print(f\"Layer Norm Output: \\n {normalized_output}\")\n",
    "mean_after_norm = normalized_output.mean(dim = -1, keepdim = True)\n",
    "variance_after_norm = normalized_output.var(dim = -1, keepdim = True, unbiased = False)\n",
    "print(f\"Mean After Layer Norm: \\n {mean_after_norm}\")\n",
    "print(f\"Variance After Layer Norm: \\n {variance_after_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b110f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
